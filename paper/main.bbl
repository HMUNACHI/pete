\begin{thebibliography}{10}

\bibitem{bertschinger2013quantifying}
Nils Bertschinger, Jan Rauh, Eckehard Olbrich, Juliane Jost, and Nihat Ay.
\newblock Quantifying unique information.
\newblock {\em Entropy}, 15(10):3508--3523, 2013.

\bibitem{snli}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In {\em Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, 2015.

\bibitem{cai2023dimensionlifting}
Borui Cai, Yong Xiang, Longxiang Gao, Di~Wu, He~Zhang, Jiong Jin, and Tom Luan.
\newblock From wide to deep: Dimension lifting network for parameter-efficient
  knowledge graph embedding.
\newblock {\em arXiv preprint arXiv:2303.12816}, 2023.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL)}, 2019.

\bibitem{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock {\em arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{liu2020rmsnorm}
Zhe Liu, Yixuan Lin, Zhiyuan Yang, Peng Zhou, and Meizhu Sun.
\newblock Root mean square layer normalization.
\newblock {\em arXiv preprint arXiv:1910.07467}, 2020.

\bibitem{loshchilov2019decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Christine Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, and Jack
  Clark.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \url{https://arxiv.org/abs/2103.00020}, 2021.
\newblock arXiv preprint arXiv:2103.00020.

\bibitem{rajbhandari2020zero}
Samyam Rajbhandari, Jordan Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimization towards training trillion parameter models.
\newblock {\em arXiv preprint arXiv:1910.02054}, 2020.

\bibitem{sennrich2016neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In {\em Proceedings of the 54th Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2016.

\bibitem{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{shen2020qbert}
Sheng Shen, Zhen Zhang, et~al.
\newblock Q-bert: Hessian-based ultra low precision quantization of bert.
\newblock In {\em Proceedings of the Thirty-Fourth AAAI Conference on
  Artificial Intelligence}, 2020.

\bibitem{shu2017compressing}
Ronghang Shu and Hideki Nakayama.
\newblock Compressing word embeddings via deep compositional code learning.
\newblock {\em arXiv preprint arXiv:1708.01079}, 2017.

\bibitem{su2021rotary}
Jun Su, Yiming Lu, Peng Huang, Cheng Xiong, and Jie Zhou.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock In {\em arXiv preprint arXiv:2104.09864}, 2021.

\bibitem{svenstrup2017hash}
Dan Svenstrup and Jes~Frellsen Hansen.
\newblock Hash embeddings for efficient word representations.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{wang2020structured}
Wang and Others.
\newblock Structured embedding compression, 2020.

\bibitem{mnli}
Adam Williams, Nandini Nangia, and Samuel~R Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, 2017.

\bibitem{williams2010nonnegative}
Paul~L Williams and Randall~D Beer.
\newblock Nonnegative decomposition of multivariate information.
\newblock {\em arXiv preprint arXiv:1004.2515}, 2010.

\bibitem{xu2023tensorgpt}
Mingxue Xu, Yao~Lei Xu, and Danilo~P. Mandic.
\newblock Tensorgpt: Efficient compression of the embedding layer in llms based
  on the tensor-train decomposition.
\newblock {\em arXiv preprint arXiv:2307.00526}, 2023.

\bibitem{yan2021adaptivemaskedtwins}
Yan and Others.
\newblock Adaptively-masked twins-based layer for efficient embeddings, 2021.

\end{thebibliography}
